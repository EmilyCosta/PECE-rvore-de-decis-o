{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamento do modelo baseado em árvore de decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "Melhores hiperparâmetros: {'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Precisão: 0.8462962962962963\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "mnist = load_digits()\n",
    "X, y = scaler.fit_transform(mnist.data), mnist.target\n",
    "X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "param = {\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Melhores hiperparâmetros: {best_params}')\n",
    "\n",
    "best_clf = grid_search.best_estimator_\n",
    "y_pred = best_clf.predict(X_test)\n",
    "\n",
    "# Imprimir a precisão da Decision Tree\n",
    "print(\"Precisão:\", best_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliação dos ganhos com a utilização de modelos Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdaBoostClassifier, RandomForestClassifier, VotingClassifier\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m      8\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "mnist = load_digits()\n",
    "X, y = scaler.fit_transform(mnist.data), mnist.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#Adaboost\n",
    "ada_clf = AdaBoostClassifier(n_estimators=50, algorithm='SAMME',random_state=42)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "#XGBoost\n",
    "xgb_clf = XGBClassifier(n_estimators=50, eval_metric='mlogloss', random_state=42)\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "#Random Forest\n",
    "rf_clf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "#Enseble\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('ada', ada_clf),\n",
    "        ('xgb', xgb_clf),\n",
    "        ('rf', rf_clf)\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy of Ensemble model: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualização da árvore de decisão e Medida de Impureza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Carregar o conjunto de dados load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "# Padronizar os dados\n",
    "scaler = StandardScaler()\n",
    "X, y = scaler.fit_transform(digits.data), digits.target\n",
    "\n",
    "# Dividir os dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Treinar a árvore de decisão\n",
    "clf = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Avaliar o desempenho do modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Precisão: {accuracy:.4f}')\n",
    "\n",
    "# Visualizar a árvore de decisão usando Matplotlib\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(clf, filled=True, feature_names=[f'pixel_{i}' for i in range(X.shape[1])], class_names=[str(i) for i in range(10)])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Função para calcular a impureza de Gini\n",
    "def gini_impurity(labels):\n",
    "    _, counts = np.unique(labels, return_counts=True)\n",
    "    probabilities = counts / len(labels)\n",
    "    return 1 - np.sum(probabilities ** 2)\n",
    "\n",
    "# Função para calcular a entropia\n",
    "def entropy(labels):\n",
    "    _, counts = np.unique(labels, return_counts=True)\n",
    "    probabilities = counts / len(labels)\n",
    "    return -np.sum(probabilities * np.log2(probabilities))\n",
    "\n",
    "# Exemplo de conjuntos de rótulos\n",
    "labels_example_1 = [0, 0, 1, 1, 1, 0, 0, 0]  # Mais homogêneo\n",
    "labels_example_2 = [0, 1, 0, 1, 0, 1, 0, 1]  # Menos homogêneo\n",
    "\n",
    "print(f'Gini Impurity (Example 1): {gini_impurity(labels_example_1):.4f}')\n",
    "print(f'Entropy (Example 1): {entropy(labels_example_1):.4f}')\n",
    "\n",
    "print(f'Gini Impurity (Example 2): {gini_impurity(labels_example_2):.4f}')\n",
    "print(f'Entropy (Example 2): {entropy(labels_example_2):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar a árvore de decisão com poda\n",
    "clf = DecisionTreeClassifier(max_depth=5, min_samples_leaf=5, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Avaliar o desempenho do modelo no conjunto de teste\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(f'Accuracy after pruning: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "model_path = 'models/model.pkl'\n",
    "joblib.dump(voting_clf, model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
